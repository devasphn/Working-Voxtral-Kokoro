# ULTRA-LOW LATENCY Configuration for <500ms end-to-end
server:
  host: "0.0.0.0"
  http_port: 8000
  health_port: 8005
  tcp_ports: [] # DISABLED: Remove unused ports

model:
  name: "mistralai/Voxtral-Mini-3B-2507"
  cache_dir: "./model_cache"
  device: "cuda"
  torch_dtype: "float16" # CRITICAL: Maximum speed
  max_memory_per_gpu: "8GB" # INCREASED: Allow more memory for speed
  # ULTRA-FAST settings
  ultra_fast_mode: true
  warmup_enabled: true

audio:
  sample_rate: 16000
  chunk_size: 1024 # OPTIMIZED: Good balance
  format: "int16"
  channels: 1
  frame_duration_ms: 20 # FAST: 2x faster than before

# ULTRA-AGGRESSIVE VAD for <500ms
vad:
  threshold: 0.005 # ULTRA-SENSITIVE: 2x more sensitive
  min_voice_duration_ms: 200 # ULTRA-FAST: 2.5x faster detection
  min_silence_duration_ms: 400 # ULTRA-FAST: 2x faster completion
  chunk_size_ms: 20 # ULTRA-SMALL: Minimal chunks
  overlap_ms: 2 # ULTRA-MINIMAL: Tiny overlap
  sensitivity: "ultra_high" # Maximum sensitivity

spectrogram:
  n_mels: 64 # REDUCED: 2x smaller for speed (was 128)
  hop_length: 160
  win_length: 320 # REDUCED: Faster processing (was 400)
  n_fft: 512 # REDUCED: 2x smaller for speed (was 1024)

streaming:
  enabled: true
  chunk_mode: "sentence_streaming" # Stream sentence by sentence
  max_connections: 50 # REDUCED: Focus resources (was 100)
  buffer_size: 1024 # ULTRA-SMALL: 2x smaller (was 2048)
  timeout_seconds: 300
  latency_target_ms: 50 # ULTRA-AGGRESSIVE: 10x more aggressive (was 500)

# ADDED: Chunked Streaming Configuration
chunked_response:
  enabled: true
  min_words_per_chunk: 2
  max_words_per_chunk: 8
  chunk_timeout_ms: 200
  separator_phrases: [". ", "? ", "! ", ", ", " and ", " but ", " so "]

# ULTRA-FAST TTS
tts:
  engine: "kokoro"
  default_voice: "hf_alpha"
  sample_rate: 16000 # CRITICAL: Match audio pipeline
  enabled: true
  voice: "hf_alpha"
  speed: 1.1 # SLIGHTLY FASTER: 10% speed boost
  lang_code: "h"
  # ULTRA-FAST performance
  performance:
    batch_size: 1
    max_queue_size: 2 # ULTRA-MINIMAL: No queuing (was 8)
    num_workers: 1 # SINGLE WORKER: Eliminate overhead
    target_latency_ms: 75 # ULTRA-FAST: 25% faster (was 100ms)
    quality_preset: "speed" # PRIORITIZE SPEED over quality

  # TTS Streaming settings
  streaming:
    enabled: true
    chunk_processing: true # Process TTS chunks immediately
    max_queue_size: 2
    target_latency_ms: 75

# ULTRA-FAST Speech-to-Speech
speech_to_speech:
  enabled: true
  latency_target_ms: 100 # ULTRA-AGGRESSIVE: 2x faster (was 200ms)
  buffer_size: 1024 # ULTRA-SMALL: 4x smaller
  output_format: "wav"
  quality: "speed" # PRIORITIZE SPEED
  emotional_expression: false # DISABLED: Skip emotion processing for speed

  streaming:
    enabled: true
    mode: "word_level" # FASTEST possible mode
    word_trigger_threshold: 1 # START after just 1 word
    max_tokens: 25 # ULTRA-SHORT responses
    interruption_detection: false # DISABLED: Skip for speed

# ULTRA-FAST performance monitoring
performance:
  enable_monitoring: true
  log_detailed_metrics: false # DISABLED: Reduce logging overhead
  latency_warning_threshold_ms: 500
  memory_monitoring: false # DISABLED: Reduce overhead

logging:
  level: "WARNING" # REDUCED: Less logging overhead (was INFO)
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/voxtral_streaming.log"
